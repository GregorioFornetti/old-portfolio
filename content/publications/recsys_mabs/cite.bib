@inproceedings{10.1145/3705328.3748166,
author = {Pires, Pedro R. and Azevedo, Gregorio F. and Campos, Pietro L. P. and Sereicikas, Rafael T. and Almeida, Tiago A.},
title = {Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748166},
doi = {10.1145/3705328.3748166},
abstract = {Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems that require continuous, incremental learning. A core aspect of MABs is the exploration–exploitation trade-off: choosing between exploiting items likely to be enjoyed and exploring new ones to gather information. In contextual linear bandits, this trade-off is particularly central, as many variants share the same linear regression backbone and differ primarily in their exploration strategies. Despite its prevalent use, offline evaluation of MABs is increasingly recognized for its limitations in reliably assessing exploration behavior. This study conducts an extensive offline empirical comparison of several linear MABs. Strikingly, across over 90\% of various datasets, a greedy linear model—with no type of exploration—consistently achieves top-tier performance, often outperforming or matching its exploratory counterparts. This observation is further corroborated by hyperparameter optimization, which consistently favors configurations that minimize exploration, suggesting that pure exploitation is the dominant strategy within these evaluation settings. Our results expose significant inadequacies in offline evaluation protocols for bandits, particularly concerning their capacity to reflect true exploratory efficacy. Consequently, this research underscores the urgent necessity for developing more robust assessment methodologies, guiding future investigations into alternative evaluation frameworks for interactive learning in recommender systems. The source code for our experiments is publicly available on .},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {736–745},
numpages = {10},
keywords = {Contextual Multi-Armed Bandits, Linear Bandits, Offline Evaluation, Exploration, Exploitation, Offline Recommender Systems},
location = {
},
series = {RecSys '25}
}